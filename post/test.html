<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Andrea | Post</title>
  <link rel="stylesheet" href="/assets/css/main.css">
</head>
<body>
  <header>
    <h1>Andrea Cardillo</h1>
    <p>Mathematician... and Python Developer <!-- | <a href="mailto:a.cardillo@sunrise.ch">a.cardillo@sunrise.ch</a> --></p>
    <nav>
      <!-- Optional: Add navigation links -->
    </nav>
  </header>
  <main>
    <!-- 
        WORK in PROGRESS
    -->
    <!--
    <section style="text-align:center;font-size:16px;color:red;">
        ðŸš§ <b>Work in progress!</b> ðŸš§ 
    </section>
    --> <!-- &#x1F6A7; -->
    <!-- 
        BLOG      
    -->
<article class="blog-post">
    <header>
        <h1>NumPy Extreme: Compiling with Optimized BLAS for Maximum Performance</h1> 
        <p class="post-meta">Published: <span>10/10/2025</span> | Author: Andrea Cardillo</p>
    </header>

  <section id="introduzione">
        <h2>Abstract</h2>
        <div class="abstract">
            <p>Installing NumPy via standard package managers (like pip) often relies on generic implementations of the basic mathematical libraries (BLAS/LAPACK). For CPU-intensive calculations, this can seriously limit performance. This article guides you through the advanced process of compiling NumPy directly from source, linking it to optimized BLAS libraries such as OpenBLAS or Intel MKL. Following this procedure is crucial for maximizing the speed of linear algebra operations on high-performance systems.</p>
        </div>

  </section>  
  <section id="introduzione">
        <h2>Keywords</h2>
        <p>NumPy, BLAS, LAPACK, OpenBLAS, Intel MKL, Compilation, Performance, HPC, Python, Linear Algebra.</p>

    </section>

    <section id="analisi-dettagliata">
        <h2>Introduction: Why Compile From Source?</h2>
        <p>NumPy is the foundation of scientific computing in Python. Much of its speed comes from delegating linear algebra operations (like matrix multiplication) to underlying libraries written in C or Fortran, known as BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra PACKage).</p>

        <p>The Complementary Roles of BLAS and LAPACK</p>
        <ol>
          <li>BLAS (The Foundation): Provides the routines for fundamental operations (vector-vector, matrix-vector, and the critical Matrix-Matrix multiplication).</li>
          <li>LAPACK (The Builder): Relies entirely on BLAS to execute higher-level, more complex operations, such as Singular Value Decomposition (SVD), LU Decomposition, and solving systems of linear equations.</li>
        </ol>        
        <p>Pre-compiled versions of NumPy often use unoptimized BLAS/LAPACK implementations. By replacing the underlying BLAS library with specialized versions like OpenBLAS or Intel MKL, we not only optimize individual matrix multiplications but automatically accelerate all the complex LAPACK routines that rely on them.</p>
    </section>

    <section id="conclusione">
        <h2>BLAS Operation Classes: What Are We Optimizing?</h2>
        <p>BLAS classifies operations based on their computational dimension. Understanding this classification is crucial to realizing why optimization is targeted:</p>
<table>
  <caption>
    BLAS operation classes
  </caption>
  <thead>
    <tr>
      <th scope="col">Class</th>
      <th scope="col">Description</th>
      <th scope="col">Complexity</th>
      <th scope="col">Performance Impact</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">1</th>
      <td>Vector-Vector (i.e. Dot Product)</td>
      <td>O(n)</td>
      <td>Often limited by memory read/write speed (Memory-bound)</td>
    </tr>
    <tr>
      <th scope="row">2</th>
      <td>Matrix-Vector (i.e. Matrix-Vector Multiplication)</td>
      <td>O(n^2)</td>
      <td>Better compute/memory ratio, but still sensitive to latency.</td>
    </tr>
    <tr>
      <th scope="row">3</th>
      <td>Matrix-Matrix (i.e. GEMM)</td>
      <td>O(n^3)</td>
      <td>The maximum gain is here. The operation is limited by pure computational power (Compute-bound).</td>
    </tr>
  </tbody>
</table>
        <p>Optimized libraries focus their effort on Class 3 (GEMM) because, for large matrices, the cubic computation time (O(n3)) far exceeds data access time, allowing the CPU to operate at its maximum capacity.</p>  
    </section>


    <section id="introduzione">
        <h2>Step 1: Installing Prerequisites</h2>
        <p>Before starting, ensure you have all the necessary development tools and Git to clone the NumPy repository.</p>
      

      <strong>A. Essential Tools</strong>
      <p>Install the Fortran compiler (necessary for BLAS/LAPACK) and development packages:</p>
     
        <pre class="line-numbers">
            <code class="language-bash">
$ sudo apt update
$ sudo apt install build-essential gfortran python3-dev git
            </code>
        </pre>
  
      <strong>B. Installing OpenBLAS (Local User)</strong>
        <pre class="line-numbers">
            <code class="language-bash">
cd /tmp
git clone --depth 1 https://github.com/OpenMathLib/OpenBLAS.git
cd OpenBLAS
OPENBLAS_INSTALL_DIR="$HOME/local/openblas"
mkdir -p $OPENBLAS_INSTALL_DIR
make -j $(nproc) USE_OPENMP=1 DYNAMIC_ARCH=1 FC=gfortran
make install PREFIX=$OPENBLAS_INSTALL_DIR 
unset OPENBLAS_INSTALL_DIR
rm -rf /tmp/OpenBLAS           
            </code>
        </pre>
<p>The OpenBLAS library is now installed in ~/local/openblas/.</p>
      <strong>C. Installing OpenBLAS (Local User)</strong>
      <p>Edit the user profile, i.e. the .bashrc, to add the environment variables</p>
        <pre class="line-numbers">
            <code class="language-bash">
# make OpenBLAS accessible to the linker at runtime 
export PKG_CONFIG_PATH="$HOME/local/openblas/lib/pkgconfig:$PKG_CONFIG_PATH"
export LD_LIBRARY_PATH="$HOME/local/openblas/lib:$LD_LIBRARY_PATH"
export BLAS_LIBS="-L$HOME/local/openblas/lib -lopenblas"
export LAPACK_LIBS="-L$HOME/local/openblas/lib -lopenblas"
export FC=gfortran # force the choice of the compiler
# specify the amount of threads - you can change it!
export OPENBLAS_NUM_THREADS="4"
export GOTO_NUM_THREADS="4"
export OMP_NUM_THREADS="4"
            </code>
        </pre>

<strong>OpenBLAS and OpenMP: Parallelization</strong>

<p>OpenBLAS achieves its superior performance through the aggressive use of the OpenMP (Open Multi-Processing) framework.</p>

<p>OpenMP is an API that supports parallel programming in shared-memory environments (your CPU cores). When OpenBLAS executes intensive Class 3 (GEMM) operations:</p>
<ol>
    <li>It uses OpenMP directives to split the matrix multiplication operation into smaller, independent blocks.</li>
    <li>These blocks are distributed as parallel tasks across all available logical cores.</li>
</ol>
<p>In this way, OpenBLAS acts as the threading engine that turns a single-threaded NumPy calculation into a multi-core operation, ensuring that your linear algebra calculations are limited by your processing power, not the software.</p>
    </section>

    <section id="introduzione">
        <h2>Step 2: Configuring NumPy's backend for BLAS</h2>
        <p>NumPy uses a file named ~/.numpy-site.cfg for a local configuration (or site.cfg, for a global configuration) to know where to find the BLAS/LAPACK libraries.</p>

        <strong>Locating the BLAS libraries</strong>
        <pre class="line-numbers">
            <code class="language-bash">
cat << EOF > ~/.numpy-site.cfg
[openblas]
libraries = openblas
library_dirs = $HOME/local/openblas/lib
include_dirs = $HOME/local/openblas/include

[lapack]
libraries = openblas
library_dirs = $HOME/local/openblas/lib
include_dirs = $HOME/local/openblas/include
EOF             
            </code>
        </pre>
    </section>
      

    <section id="introduzione">
        <h2>Step 3: Preparing the Compilation Environment</h2>
        <p>Let's create a clean environment and prepare the NumPy sources.</p>

        <strong>Install NumPy in a new fresh environment</strong>
        <pre class="line-numbers">
            <code class="language-bash">
python3 -m venv vproject
(vproject) python -m pip install -U pip
(vproject) python -m pip install --no-cache numpy --no-binary numpy --verbose              
            </code>
        </pre>

        <strong>Verify Installation:</strong>
        <pre class="line-numbers">
            <code class="language-bash">
(vproject) python -c "import numpy as np; np.show_config()"
            </code>
        </pre>

    </section>

  
    <section id="introduzione">
        <h2>Conclusion: Optimization and Context</h2>
        <p>Compiling NumPy with an optimized BLAS library is not a miracle cure for every user. For developers performing standard numerical operations or working with small datasets, the performance gain compared to the version provided by pip can be minimal or negligible.</p>

        <p>However, this process is essential and offers tangible value in specific contexts:</p>
        <ol>
           <li>HPC/Scientific Workloads: For those managing intensive linear algebra calculations, such as large matrix analysis, scientific simulations, or machine learning model training on dedicated hardware, optimizing the machine code (by exploiting AVX/FMA instructions) can lead to significant time savings that accumulate over time.</li>
            <li>Transparency and Control: This procedure ensures you have total control over the underlying dependencies, eliminating any doubt about potential bottlenecks caused by unoptimized BLAS/LAPACK.</li>
        </ol>
        <p>Ultimately, if benchmarks show that BLAS operations are your primary bottleneck, compiling from source is the only way to extract maximum efficiency from your hardware. For everyone else, the standard installation via pip remains the fastest and most practical choice.</p>
    </section>
  
    <section>
        <strong>References:</strong><p class="tags">Source 1, </p>
    </section>
</article>

<aside class="sidebar-navigation">
    <h3>Indice dei Post</h3>
    <nav>
        <ul>
            <li><a href="/post-a">Post Correlato A</a></li>
            </ul>
    </nav>
</aside>

    </main>
      <!-- 
          FOOTER      
      -->  
    <footer>
      &copy; 2025 Andrea Cardillo
    </footer>
     
</body>
</html>
